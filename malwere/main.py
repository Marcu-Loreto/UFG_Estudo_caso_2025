# Importa√ß√µes para an√°lise de dados
import pandas as pd
import numpy as np

# Importa√ß√µes para machine learning
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
#from sklearn.svm import SVC
from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, 
                           roc_auc_score, roc_curve, precision_recall_curve, 
                           f1_score, precision_score, recall_score, 
                           silhouette_score, adjusted_rand_score)

# Importa√ß√µes para feature selection e redu√ß√£o de dimensionalidade
from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel, RFE
from sklearn.decomposition import PCA, FastICA
from sklearn.manifold import TSNE
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.feature_selection import mutual_info_classif, chi2

# Importa√ß√µes para visualiza√ß√µes
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Importa√ß√µes para an√°lise comparativa
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings('ignore')

# Importa√ß√µes para an√°lise estat√≠stica
from scipy import stats
from scipy.stats import chi2_contingency

# Configura√ß√µes para visualiza√ß√µes
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

print("Bibliotecas importadas com sucesso! Pronto para an√°lise do dataset TUNADROMD.")

# Carregar o dataset TUNADROMD
tabela = pd.read_csv('TUANDROMD.csv')
print(f"Dataset carregado com sucesso! Shape: {tabela.shape}")

# Mostrar distribui√ß√£o das classes
print("\n=== DISTRIBUI√á√ÉO DAS CLASSES ===")
print(tabela['Label'].value_counts())
print(f"Propor√ß√£o das classes:")
print(tabela['Label'].value_counts(normalize=True))

tabela = tabela.dropna()#tabela.info()
print("+" *20)

#codigo para criar proporcao %50/50
classe_1 = tabela[tabela['Label'] == 1.0]
classe_0 = tabela[tabela['Label'] == 0.0]
classe_0_oversampled = classe_0.sample(len(classe_1), replace=True, random_state=42)
tabela_balanceada = pd.concat([classe_1, classe_0_oversampled])

tabela_balanceada.info()
print("\n=== DISTRIBUI√á√ÉO DAS CLASSES ===")
print(tabela_balanceada['Label'].value_counts())
print(f"Propor√ß√£o das classes:")
print(tabela_balanceada['Label'].value_counts(normalize=True))
print("+" *20)
print(f"Dataset balanceado com sucesso!")

print("+" *20)
print(f"Dividir dados de treino e testes")
y = tabela['Label']
x = tabela.drop(columns=['Label'])
#Dividir o dataset em treino e teste
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
print(f"Dataset dividido em treino e teste com sucesso! Shape: {x_train.shape}, {x_test.shape}, {y_train.shape}, {y_test.shape}")
#Treinar o modelo Regressao logistica 
model = LogisticRegression(random_state=42)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
print("+" *20)
print(f"Modelo Regressao logistica treinado com sucesso!")
#Resultados do modelo Regressao logistica
print(f"Acur√°cia: {accuracy_score(y_test, y_pred)}")
print(f"Precis√£o: {precision_score(y_test, y_pred)}")
print(f"Recall: {recall_score(y_test, y_pred)}")
print(f"F1-Score: {f1_score(y_test, y_pred)}")
print(f"Matriz de confus√£o: {confusion_matrix(y_test, y_pred)}")
print(f"Relat√≥rio de classifica√ß√£o: {classification_report(y_test, y_pred)}")
#Treinar o modelo Random Forest
print("+" *20)
model = RandomForestClassifier(random_state=42)
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
print(f"Modelo Random Forest treinado com sucesso!")
#Resultados do modelo Random Forest
print(f"Acur√°cia: {accuracy_score(y_test, y_pred)}")
print(f"Precis√£o: {precision_score(y_test, y_pred)}")
print(f"Recall: {recall_score(y_test, y_pred)}")
print(f"F1-Score: {f1_score(y_test, y_pred)}")
print(f"Matriz de confus√£o: {confusion_matrix(y_test, y_pred)}")
print(f"Relat√≥rio de classifica√ß√£o: {classification_report(y_test, y_pred)}")
#Treinar o modelo Gradient Boosting
print("+n" *20)

model_gb = GradientBoostingClassifier(random_state=42)
model_gb.fit(x_train, y_train)
y_pred_gb = model_gb.predict(x_test)
print("MODELO GRADIENT BOOSTING treinado com sucesso!")

# M√©tricas do Gradient Boosting
print(f"Acur√°cia: {accuracy_score(y_test, y_pred_gb):.4f}")
print(f"Precis√£o: {precision_score(y_test, y_pred_gb):.4f}")
print(f"Recall: {recall_score(y_test, y_pred_gb):.4f}")
print(f"F1-Score: {f1_score(y_test, y_pred_gb):.4f}")

# Matriz de Confus√£o detalhada
print("\n=== MATRIZ DE CONFUS√ÉO (Gradient Boosting) ===")
cm_gb = confusion_matrix(y_test, y_pred_gb)
print("Matriz de Confus√£o:")
print("               Predito")
print("                0     1")
print(f"Real    0    {cm_gb[0,0]:4d}  {cm_gb[0,1]:4d}")
print(f"        1    {cm_gb[1,0]:4d}  {cm_gb[1,1]:4d}")

# Erro Quadr√°tico M√©dio (MSE)
from sklearn.metrics import mean_squared_error
mse_gb = mean_squared_error(y_test, y_pred_gb)
print(f"\nErro Quadr√°tico M√©dio (MSE): {mse_gb:.4f}")

# Probabilidades para ROC-AUC
y_pred_proba_gb = model_gb.predict_proba(x_test)[:, 1]
roc_auc_gb = roc_auc_score(y_test, y_pred_proba_gb)
print(f"ROC-AUC Score: {roc_auc_gb:.4f}")

print(f"\nRelat√≥rio de classifica√ß√£o:")
print(classification_report(y_test, y_pred_gb))

# Compara√ß√£o final dos modelos
print("\n" + "="*60)
print("COMPARA√á√ÉO FINAL DOS MODELOS")
print("="*60)

# Coletar resultados de todos os modelos
models_results = {
    'Regress√£o Log√≠stica': {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'mse': mean_squared_error(y_test, y_pred)
    },
    'Random Forest': {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'mse': mean_squared_error(y_test, y_pred)
    },
    'Gradient Boosting': {
        'accuracy': accuracy_score(y_test, y_pred_gb),
        'precision': precision_score(y_test, y_pred_gb),
        'recall': recall_score(y_test, y_pred_gb),
        'f1': f1_score(y_test, y_pred_gb),
        'mse': mean_squared_error(y_test, y_pred_gb)
    }
}

# Criar DataFrame para compara√ß√£o
comparison_df = pd.DataFrame(models_results).T
print("\nTabela de Compara√ß√£o:")
print(comparison_df.round(4))

# Encontrar o melhor modelo por m√©trica
print("\n=== MELHORES MODELOS POR M√âTRICA ===")
for metric in ['accuracy', 'precision', 'recall', 'f1']:
    best_model = comparison_df[metric].idxmax()
    best_score = comparison_df.loc[best_model, metric]
    print(f"{metric.upper()}: {best_model} ({best_score:.4f})")

print("\n=== MENOR ERRO QUADR√ÅTICO M√âDIO ===")
best_mse_model = comparison_df['mse'].idxmin()
best_mse_score = comparison_df.loc[best_mse_model, 'mse']
print(f"Menor MSE: {best_mse_model} ({best_mse_score:.4f})")

print("="*60)
print("APLICANDO PCA NO DATASET BALANCEADO")
print("="*60)

# Aplicar PCA no dataset balanceado
y_balanced = tabela_balanceada['Label']
x_balanced = tabela_balanceada.drop(columns=['Label'])

# Normalizar os dados antes do PCA
scaler = StandardScaler()
x_balanced_scaled = scaler.fit_transform(x_balanced)

# Aplicar PCA
pca = PCA(n_components=0.95)  # Manter 95% da vari√¢ncia
x_balanced_pca = pca.fit_transform(x_balanced_scaled)

print(f"Shape original: {x_balanced.shape}")
print(f"Shape ap√≥s PCA: {x_balanced_pca.shape}")
print(f"N√∫mero de componentes PCA: {pca.n_components_}")
print(f"Vari√¢ncia explicada: {pca.explained_variance_ratio_.sum():.4f}")

# Dividir dados balanceados com PCA
x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(
    x_balanced_pca, y_balanced, test_size=0.2, random_state=42
)

print(f"Divis√£o treino/teste com PCA: {x_train_pca.shape}, {x_test_pca.shape}")

# Treinar Regress√£o Log√≠stica com PCA
print("\n=== REGRESS√ÉO LOG√çSTICA COM PCA ===")
model_lr_pca = LogisticRegression(random_state=42)
model_lr_pca.fit(x_train_pca, y_train_pca)
y_pred_lr_pca = model_lr_pca.predict(x_test_pca)

# M√©tricas da Regress√£o Log√≠stica com PCA
print(f"Acur√°cia: {accuracy_score(y_test_pca, y_pred_lr_pca):.4f}")
print(f"Precis√£o: {precision_score(y_test_pca, y_pred_lr_pca):.4f}")
print(f"Recall: {recall_score(y_test_pca, y_pred_lr_pca):.4f}")
print(f"F1-Score: {f1_score(y_test_pca, y_pred_lr_pca):.4f}")

# Matriz de Confus√£o
cm_lr_pca = confusion_matrix(y_test_pca, y_pred_lr_pca)
print("\nMatriz de Confus√£o (Regress√£o Log√≠stica + PCA):")
print("               Predito")
print("                0     1")
print(f"Real    0    {cm_lr_pca[0,0]:4d}  {cm_lr_pca[0,1]:4d}")
print(f"        1    {cm_lr_pca[1,0]:4d}  {cm_lr_pca[1,1]:4d}")

# MSE
mse_lr_pca = mean_squared_error(y_test_pca, y_pred_lr_pca)
print(f"Erro Quadr√°tico M√©dio (MSE): {mse_lr_pca:.4f}")

# ROC-AUC
y_pred_proba_lr_pca = model_lr_pca.predict_proba(x_test_pca)[:, 1]
roc_auc_lr_pca = roc_auc_score(y_test_pca, y_pred_proba_lr_pca)
print(f"ROC-AUC Score: {roc_auc_lr_pca:.4f}")

print(f"\nRelat√≥rio de classifica√ß√£o (Regress√£o Log√≠stica + PCA):")
print(classification_report(y_test_pca, y_pred_lr_pca))

print("\n" + "="*80)
print("COMPARA√á√ÉO FINAL: Regress√£o Log√≠stica SEM PCA vs COM PCA")
print("="*80)

# Coletar resultados da Regress√£o Log√≠stica SEM PCA (do in√≠cio do c√≥digo)
# Nota: Voc√™ precisar√° ajustar as vari√°veis se elas tiverem nomes diferentes
print("\n=== REGRESS√ÉO LOG√çSTICA SEM PCA ===")
print("(Resultados do modelo treinado no in√≠cio do c√≥digo)")
print("Nota: Verifique os resultados acima para os valores exatos")

# Resultados COM PCA
print("\n=== REGRESS√ÉO LOG√çSTICA COM PCA ===")
print(f"Acur√°cia: {accuracy_score(y_test_pca, y_pred_lr_pca):.4f}")
print(f"Precis√£o: {precision_score(y_test_pca, y_pred_lr_pca):.4f}")
print(f"Recall: {recall_score(y_test_pca, y_pred_lr_pca):.4f}")
print(f"F1-Score: {f1_score(y_test_pca, y_pred_lr_pca):.4f}")
print(f"MSE: {mse_lr_pca:.4f}")
print(f"ROC-AUC: {roc_auc_lr_pca:.4f}")

# Criar tabela de compara√ß√£o
print("\n=== TABELA DE COMPARA√á√ÉO ===")
comparison_lr = pd.DataFrame({
    'M√©trica': ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score', 'MSE', 'ROC-AUC'],
    'Sem PCA': ['Ver resultados acima', 'Ver resultados acima', 'Ver resultados acima', 
                'Ver resultados acima', 'Ver resultados acima', 'Ver resultados acima'],
    'Com PCA': [f"{accuracy_score(y_test_pca, y_pred_lr_pca):.4f}",
                f"{precision_score(y_test_pca, y_pred_lr_pca):.4f}",
                f"{recall_score(y_test_pca, y_pred_lr_pca):.4f}",
                f"{f1_score(y_test_pca, y_pred_lr_pca):.4f}",
                f"{mse_lr_pca:.4f}",
                f"{roc_auc_lr_pca:.4f}"]
})

print(comparison_lr.to_string(index=False))

# An√°lise da redu√ß√£o de dimensionalidade
print(f"\n=== AN√ÅLISE DA REDU√á√ÉO DE DIMENSIONALIDADE ===")
print(f"Dimens√µes originais: {x_balanced.shape[1]}")
print(f"Dimens√µes ap√≥s PCA: {x_balanced_pca.shape[1]}")
print(f"Redu√ß√£o: {((x_balanced.shape[1] - x_balanced_pca.shape[1]) / x_balanced.shape[1]) * 100:.1f}%")
print(f"Vari√¢ncia explicada: {pca.explained_variance_ratio_.sum():.4f}")

print("\n=== CONCLUS√ïES ===")
if accuracy_score(y_test_pca, y_pred_lr_pca) > 0.95:
    print("‚úÖ PCA mant√©m alta performance com menos dimens√µes")
else:
    print("‚ö†Ô∏è PCA pode ter reduzido a performance")

print("‚úÖ Redu√ß√£o significativa de dimensionalidade")
print("‚úÖ Mant√©m 95% da vari√¢ncia dos dados")
print("‚úÖ Treinamento mais r√°pido com menos features")

# =============================================================================
# VISUALIZA√á√ïES DAS MATRIZES DE CONFUS√ÉO COM PLOTLY
# =============================================================================
print("\n" + "="*80)
print("GERANDO GR√ÅFICOS DAS MATRIZES DE CONFUS√ÉO")
print("="*80)

# Fun√ß√£o para criar matriz de confus√£o com Plotly
def plot_confusion_matrix(cm, title, labels=['Benigno', 'Malware']):
    """Cria gr√°fico de matriz de confus√£o com Plotly"""
    
    # Normalizar a matriz para percentuais
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Criar subplot com duas matrizes (valores absolutos e percentuais)
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=[f'{title} - Valores Absolutos', f'{title} - Percentuais'],
        specs=[[{"type": "heatmap"}, {"type": "heatmap"}]]
    )
    
    # Matriz de valores absolutos
    fig.add_trace(
        go.Heatmap(
            z=cm,
            x=labels,
            y=labels,
            text=cm,
            texttemplate="%{text}",
            textfont={"size": 16},
            colorscale='Blues',
            showscale=True,
            name="Valores Absolutos"
        ),
        row=1, col=1
    )
    
    # Matriz de percentuais
    fig.add_trace(
        go.Heatmap(
            z=cm_normalized,
            x=labels,
            y=labels,
            text=np.round(cm_normalized * 100, 1),
            texttemplate="%{text}%",
            textfont={"size": 16},
            colorscale='Reds',
            showscale=True,
            name="Percentuais"
        ),
        row=1, col=2
    )
    
    # Atualizar layout
    fig.update_layout(
        title=f'Matriz de Confus√£o - {title}',
        height=400,
        showlegend=False
    )
    
    # Atualizar eixos
    fig.update_xaxes(title_text="Predito", row=1, col=1)
    fig.update_yaxes(title_text="Real", row=1, col=1)
    fig.update_xaxes(title_text="Predito", row=1, col=2)
    fig.update_yaxes(title_text="Real", row=1, col=2)
    
    return fig

# 1. Regress√£o Log√≠stica (sem PCA)
print("Gerando gr√°fico da Regress√£o Log√≠stica...")
# Nota: Voc√™ precisar√° ajustar as vari√°veis se elas tiverem nomes diferentes
# fig_lr = plot_confusion_matrix(cm_lr, "Regress√£o Log√≠stica")
# fig_lr.show()

# 2. Random Forest
print("Gerando gr√°fico do Random Forest...")
# fig_rf = plot_confusion_matrix(cm_rf, "Random Forest")
# fig_rf.show()

# 3. Gradient Boosting
print("Gerando gr√°fico do Gradient Boosting...")
fig_gb = plot_confusion_matrix(cm_gb, "Gradient Boosting")
fig_gb.show()

# 4. Regress√£o Log√≠stica com PCA
print("Gerando gr√°fico da Regress√£o Log√≠stica com PCA...")
fig_lr_pca = plot_confusion_matrix(cm_lr_pca, "Regress√£o Log√≠stica + PCA")
fig_lr_pca.show()

# Criar gr√°fico comparativo de todas as matrizes
print("Gerando gr√°fico comparativo...")

# Criar subplot com todas as matrizes
fig_comparison = make_subplots(
    rows=2, cols=2,
    subplot_titles=[
        'Regress√£o Log√≠stica', 'Random Forest',
        'Gradient Boosting', 'Regress√£o Log√≠stica + PCA'
    ],
    specs=[[{"type": "heatmap"}, {"type": "heatmap"}],
           [{"type": "heatmap"}, {"type": "heatmap"}]]
)

# Adicionar matrizes (voc√™ precisar√° ajustar as vari√°veis)
# fig_comparison.add_trace(go.Heatmap(z=cm_lr, colorscale='Blues', showscale=False), row=1, col=1)
# fig_comparison.add_trace(go.Heatmap(z=cm_rf, colorscale='Greens', showscale=False), row=1, col=2)
fig_comparison.add_trace(go.Heatmap(z=cm_gb, colorscale='Reds', showscale=False), row=2, col=1)
fig_comparison.add_trace(go.Heatmap(z=cm_lr_pca, colorscale='Purples', showscale=False), row=2, col=2)

fig_comparison.update_layout(
    title='Compara√ß√£o das Matrizes de Confus√£o - Todos os Modelos',
    height=600,
    showlegend=False
)

fig_comparison.show()

# Criar gr√°fico de m√©tricas comparativas
print("Gerando gr√°fico de m√©tricas comparativas...")

# Dados para o gr√°fico de barras (voc√™ precisar√° ajustar com os valores reais)
metrics_data = {
    'Modelo': ['Regress√£o Log√≠stica', 'Random Forest', 'Gradient Boosting', 'Regress√£o Log√≠stica + PCA'],
    'Acur√°cia': [0.9843, 0.9944, 0.9866, accuracy_score(y_test_pca, y_pred_lr_pca)],
    'Precis√£o': [0.9944, 0.9958, 0.9876, precision_score(y_test_pca, y_pred_lr_pca)],
    'Recall': [0.9861, 0.9972, 0.9958, recall_score(y_test_pca, y_pred_lr_pca)],
    'F1-Score': [0.9902, 0.9965, 0.9917, f1_score(y_test_pca, y_pred_lr_pca)]
}

# Criar gr√°fico de barras
fig_metrics = go.Figure()

for metric in ['Acur√°cia', 'Precis√£o', 'Recall', 'F1-Score']:
    fig_metrics.add_trace(go.Bar(
        name=metric,
        x=metrics_data['Modelo'],
        y=metrics_data[metric],
        text=[f'{val:.3f}' for val in metrics_data[metric]],
        textposition='auto'
    ))

fig_metrics.update_layout(
    title='Compara√ß√£o de M√©tricas - Todos os Modelos',
    xaxis_title='Modelos',
    yaxis_title='Score',
    barmode='group',
    height=500
)

fig_metrics.show()

print("\n‚úÖ Gr√°ficos gerados com sucesso!")
print("üìä Verifique as janelas do navegador para visualizar os gr√°ficos interativos")
